Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kross2017,
annote = {give an overview of their coursera class

thought some of their metrics were questionable --- do we really know how well students learned the material? Or how much students really learned? seems like they spent a lot of time talking about enrollment numbers, which doesn't really mean anything.

also, this really doesn't imply the democratization of data science. which they admit at the end. but i wonder what the demographics of the people who take their classes are. 

also, what does "the democratization of data demands a democratization of data science eduation mean" -- why does it demand it? there are plenty of things that are democratized that do not imply that education in that field must be democratized (or is democratized)

also skeptical that coursera is the answer. true democratization would probably look more radical --- i.e., incorporating data science into middle/ high school curriculum in public schools},
author = {Kross, Sean and Peng, Roger D and Caffo, Brian S and Gooding, Ira and Leek, Jeffrey T},
doi = {10.7287/peerj.preprints.3195v1},
file = {:Users/sarakai24/Downloads/peerj-preprints-3195.pdf:pdf},
issn = {2167-9843},
journal = {PeerJ Preprints},
title = {{​ ​ The ​democratization of data ​ ​ science education}},
year = {2017}
}
@article{Wilson2016,
abstract = {We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing.},
annote = {learned most of these in the context of reproducible research

think of reproducibility in science as largely composed of issues with tools 

needs to be a greater focus on these kinds of tools in social sciences -- makes everything easier and makes the science better

maybe because the topics (e.g. psych) are so divorced from the tools?},
archivePrefix = {arXiv},
arxivId = {1609.00037},
author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
doi = {10.1371/journal.pcbi.1005510},
eprint = {1609.00037},
file = {:Users/sarakai24/Downloads/journal.pcbi.1005510.pdf:pdf},
isbn = {1111111111},
issn = {1553-7358},
journal = {PLoS Computational Biology},
pages = {1--20},
pmid = {28640806},
title = {{Good Enough Practices in Scientific Computing}},
url = {http://arxiv.org/abs/1609.00037},
volume = {13},
year = {2016}
}
@misc{Patil,
annote = {calls for a code of ethics for data science, where the entire data science community contributes},
author = {Patil, DJ},
title = {{A Code of Ethics for Data Science}},
url = {https://medium.com/@dpatil/a-code-of-ethics-for-data-science-cda27d1fac1},
urldate = {2018-05-12}
}
@article{Kaplan2017,
abstract = {The familiar mathematical topics of introductory statistics --- means, proportions, t-tests, normal and t distributions, chi-squared, etc. --- are a product of the first half of the 20th century. Naturally, they reflect the statistical conditions of that era: scarce, e.g. n {\textless} 10, data originating in benchtop or agricultural experiments; algorithms communicated via algebraic formulas. Today, applied statistics relates to a different environment: software is the means of algorithmic communication, observational and "unplanned" data are interpreted for causal relationships, and data are large both in n and the number of variables. This change in situation calls for a thorough rethinking of the topics in and approach to statistics education. This paper presents a set of ten organizing blocks for intro stats that are better suited to today's environment.},
annote = {why does he think the things he says about modeling are important or worth teaching? doesn't really explain why. are these the most useful concepts? Im not srue that they are (although I don't actually know what the most useful concepts are)

talks about various "blocks" that we should teach when we teach data science. less radical than that other article about overhauling statistics education.},
author = {Kaplan, Daniel},
doi = {10.1080/00031305.2017.1398107},
file = {:Users/sarakai24/Downloads/peerj-preprints-3205.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
pages = {0--0},
title = {{Teaching Stats for Data Science}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1398107},
year = {2017}
}
@misc{Fish2018,
annote = {tracked his time---set expectations and then comapred to what he actually spent his time on

set out to reproduce a deep learning paper

how much expertise did he have in the field?
- he's a masters student in computational neuroscience

reproduces a paper as a way of improving his deep learning skills

wasn't as straightforward as just recreating what someone else had done---he had to do a lot of fiddling aroudn

he had to think a lot about what was going wrong

also takes a lot of money to reproduce this kind of work
- and time

confused if he was able to reproduce results, or what that really means in deep learning?},
author = {Fish, Amid},
title = {{Lessons Learned Reproducing a Deep Reinforcement Learning Paper}},
url = {http://amid.fish/reproducing-deep-rl},
urldate = {2018-05-03},
year = {2018}
}
@misc{Lincoln,
annote = {make it tidy-able
- make it so that it's easy to enter, but also easy to tidy (i.e. good column names)

as a data enterer and creater of spreadsheets that other people also enter data into, i found this stuff useful

also found the idea that you should mark a suspicious entry with a value and not a color not super helpful

should also be thinking of the spreadsheet in terms of UX -- there's operating in R or whatever but also using the actual spreadsheet. many people spend a lot of time using the actual spreadsheet},
author = {Lincoln, Matthew},
title = {{Best Practices for Using Google Sheets in Your Data Project - Matthew Lincoln, PhD}},
url = {https://matthewlincoln.net/2018/03/26/best-practices-for-using-google-sheets-in-your-data-project.html},
urldate = {2018-04-08}
}
@article{Sculley2014,
abstract = {Machine learning offers a fantastically powerful toolkit for building complex sys-tems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is re-markably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several ma-chine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
annote = {technical debt},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/sarakai24/Downloads/43146.pdf:pdf},
isbn = {9780874216561},
issn = {13613723},
journal = {NIPS 2014 Workshop on Software Engineering for Machine Learning (SE4ML)},
pages = {1--9},
pmid = {1000106311},
title = {{Machine Learning : The High-Interest Credit Card of Technical Debt}},
year = {2014}
}
@misc{Wallach,
annote = {says if tech companies/govt is serious about bias fairness etc they should hire social scientists

takeaways
big data = "granular, social data"
think of big data as many diverse data sets nested within a larger collection
addressing both points requires social scientists

questions
prioritze social questions over data availability to avoid focusing on readily avialble data sets or only getting the course grained patterns "evidenced by the majority"
- question driven approach is harder bc data availability, but collecting data about qeustions related to "bais, fairness, inclusion" will help everyone

models
- three types of modeling tasks: prediction, explanation, exploration
- importance of different types of errors, not just error metric
- again, identify issue from social science perspective, then solve in atehcnical manner

- good to report uncertainty along with decisions

takeaways
- models have three main functions
- to achieve fairness, need rigorous error analysis and mdoel validation
- use modeling frameworks to represent, maintain, and report uncertainty

findings
- need to make sure we are working in a fiar, accountable, and scientific fashion
- shouls work with social scientists
- example confirmation bias
takeaways
- be aware of things like implicit bias
- increase in commitment to public understanding of science},
author = {Wallach, Hanna},
title = {{Big Data, Machine Learning, and the Social Sciences}},
url = {https://medium.com/@hannawallach/big-data-machine-learning-and-the-social-sciences-927a8e20460d},
urldate = {2018-05-12}
}
@misc{Granville,
author = {Granville, Vincent},
title = {{Data science without statistics is possible, even desirable}},
url = {https://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable},
urldate = {2018-04-12}
}
@article{Derman2012,
author = {Derman, Emanuel},
doi = {10.3905/jod.2012.20.1.035},
issn = {1074-1240},
journal = {The Journal of Derivatives},
month = {aug},
number = {1},
pages = {35--37},
title = {{Apologia Pro Vita Sua}},
url = {http://jod.iijournals.com/lookup/doi/10.3905/jod.2012.20.1.035},
volume = {20},
year = {2012}
}
@article{Broman2017,
abstract = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this paper offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, don't leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, don't include calculations in the raw data files, don't use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text file.},
annote = {- I always find excel claustrophobic
- would never have realized how painful these wrong things were before extensively cleaning adn working with data
- so many problems in my own research have been caused by extra spaces that I didn't realize were present
- a lot of my current coding for my thesis involves changing lots of variable names so that they don't have spaces in an efficient way

- reproducible research adn good science
- these tools are really important in many scientific fields, but good practices and mastery sometimes aren't really emphasized, or they aren't seen as important as other aspects of science. things are getting better I think 
- these are tools that we use to do science (just as chemists and molecular biologists need good beakers and centrifuges and pipeetes) we need good data tools in order to do good science},
author = {Broman, Karl W. and Woo, Kara H.},
doi = {10.1080/00031305.2017.1375989},
file = {:Users/sarakai24/Downloads/peerj-preprints-3183.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {2126 genetics-,425 henry mall,and jenny bryan for,biostat,biotechnology center,broman,corresponding author,data management,data organization,edu,karl w,kbroman,lincoln mullen,madison,manuscript,microsoft excel,spreadsheets,the,the authors thank lance,their comments to improve,university of wisconsin,usa,waller,wi 53706,wisc},
pages = {0--0},
title = {{Data organization in spreadsheets}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989},
year = {2017}
}
@article{Brown2018,
abstract = {Research from educational psychology suggests that teaching and learning are subject-specific activities [1]: learning programming has a different set of challenges and techniques than learning physics or learning to read and write. Computing is a younger discipline than mathe- matics, physics, or biology, and while there have been correspondingly fewer studies of how best to teach it, there is a growing body of evidence about what works and what doesn't. This paper presents 10 quick tips that should be the foundation of any teaching of programming, whether formal or informal.},
annote = {give 10 tips for teaching programming
seems more oriented towards teaching computer science type programming than for data science, but still relevant

some of the things he says are unsupported -- e.g., " if you are teachign someone to program, the last thing you want to do is make them feel like ..." I agree but how do we know that this is true? and why is it that programmers have the "reputation for gatekeeping"---how did this arise?

interesting point about choosing context --- that choosing context can inadvertently exclude some people while drawing other people in 

if data science is a new field, we have the opportunity to avoid mistakes made by computer science. decisions were made and continue to be made that keep minorities and women from entering the field. we don't have to commit these same mistakes with data science.},
author = {Brown, Neil C. C. and Wilson, Greg},
doi = {10.1371/journal.pcbi.1006023},
file = {:Users/sarakai24/Downloads/journal.pcbi.1006023.pdf:pdf},
isbn = {1111111111},
issn = {1553-7358},
journal = {PLOS Computational Biology},
number = {4},
pages = {e1006023},
pmid = {29621229},
title = {{Ten quick tips for teaching programming}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1006023},
volume = {14},
year = {2018}
}
@article{Klein2018,
abstract = {The credibility of scientific claims depends upon the transparency of the research products upon which they are based (e.g., study protocols, data, materials, and analysis scripts). As psychology navigates a period of unprecedented introspection, user-friendly tools and services that support open science have flourished. There has never been a better time to embrace transparent research practices. However, the plethora of decisions and choices involved can be bewildering. Here we provide a practical guide to help researchers navigate the process of preparing and sharing the products of their research. Being an open scientist means adopting a few straightforward research management practices, which lead to less error prone, reproducible research workflows. Further, this adoption can be piecemeal – each incremental step towards complete transparency adds positive value. Transparent research practices not only improve the efficiency of individual researchers, they enhance the credibility of the knowledge generated by the scientific community.},
annote = {- see transparent research as way to fix scientific problems in pyschology

intersection between statistics and what humans do with statistics

talks about the purpose of transparency in science


"do not let the perfect be the enemy of the good" wrt sharing analyses

post preprints means it is almost universally possibel to ensure open access

when to share?
- preregistration

why do researchers not share? 

- concerns that other researchers will use their work to help themselves
- says nto really a worry in psychology
- but from what i understand may be a problem in machine learning and other fields where timing and margins in metrics are small


- say getting scooped can help with exposure of the original researcher
aside: seems like some resistance to preregistration, thinking about data sharing, etc. comes from a resistance to front load work, and the pressure to start collecting data as soon as possible},
author = {Klein, Olivier and Hardwicke, Tom E. and Aust, Frederik and Breuer, Johannes and Danielsson, Henrik and Mohr, Alicia Hofelich and IJzerman, Hans and Nilsonne, Gustav and Vanpaemel, Wolf and Frank, Michael C.},
doi = {https://dx.doi.org/10.17605/OSF.IO/RTYGM},
file = {:Users/sarakai24/Downloads/Transparency Guide.pdf:pdf},
journal = {PsyArXiv},
title = {{A practical guide for transparency in psychological science}},
year = {2018}
}
@article{Boettiger2014,
abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
annote = {2 approaches to problem of coordinating tools to make analysis reproducible:
- workflow software
- pro: works well 
- con: low adoption

- vms
- pro : more direct
- cpature OS and everything running on it
con: black box -- ill suited for reproducibility
- con: other research can't build upon teh vm in a consistent and scalable way -- might want to combine the tools of multiple studies, but this would be impractical/impossible with vms


technical barriers can influence the cultural landscape},
archivePrefix = {arXiv},
arxivId = {1410.0846},
author = {Boettiger, Carl},
doi = {10.1145/2723872.2723882},
eprint = {1410.0846},
file = {:Users/sarakai24/Downloads/1410.0846.pdf:pdf},
isbn = {0163-5980},
issn = {01635980},
title = {{An introduction to Docker for reproducible research, with examples from the R environment}},
url = {http://arxiv.org/abs/1410.0846{\%}0Ahttp://dx.doi.org/10.1145/2723872.2723882},
year = {2014}
}
@article{ASA2016,
abstract = {ASA ethical guidelines: The American Statistical Association's Ethical Guidelines for Statistical Practice are intended to help statistics practitioners make decisions ethically. Additionally, the Ethical Guidelines aim to promote ac-countability by informing those who rely on statistical analysis of the standards that they should expect.},
annote = {basic principles
- transparent assumptions
- reproducible results
- valid interpretations

goal: advance knowledge while avoiding harm

some of these seem to overlap with reproducibel science guidelines, and general science / IRB guidelines

some are just general---"uses professional qualification and contributions" to decide hiring firing etc

some are related to knowledge of the field
- "understands the differences between questionable statistical, scientific, or professional practices adn practices taht constitute misconduct"},
author = {ASA},
file = {:Users/sarakai24/Library/Application Support/Mendeley Desktop/Downloaded/ASA - 2016 - Ethical Guidelines for Statistical Practice American Statistical Association.pdf:pdf},
journal = {American Statistical Association},
number = {April},
pages = {1--12},
title = {{Ethical Guidelines for Statistical Practice: American Statistical Association}},
url = {http://www.amstat.org/about/ethicalguidelines.cfm},
year = {2016}
}
@article{Donoho2017,
abstract = {ABSTRACTMore than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a {\$}100M “Data Science Initiative” that aims to hire 35 new faculty....},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Donoho, David},
doi = {10.1080/10618600.2017.1384734},
eprint = {arXiv:1011.1669v3},
file = {:Users/sarakai24/Downloads/50 Years of Data Science.pdf:pdf},
isbn = {0672326965},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Cross-study analysis,Data analysis,Data science,Meta analysis,Predictive modeling,Quantitative programming environments,Statistics},
number = {4},
pages = {745--766},
pmid = {25246403},
publisher = {Taylor {\&} Francis},
title = {{50 Years of Data Science}},
url = {https://doi.org/10.1080/10618600.2017.1384734},
volume = {26},
year = {2017}
}
@phdthesis{Keegan,
author = {Keegan, Brian C.},
title = {{Journalism as a Professional Model for Data Science}}
}
@misc{Wheeler,
annote = {"If we can realistically expect everyone in the community to just adopt a code of ethics because thye intuitively 

how is the HO not a statement of write and wrong? 

this signaling thing will not work at all. 

definition of what should happen with a data scientist: People in a business, on average over the course of their interactions with a data scientist
- this seems wrong

agree that ehtical code means people will virtue signal

Data scientists don't need a list of ways to be virtuous. They need a list of ways to prove they aren't charlatans. That will do more to ensure the health and trustworthiness of the profession than anything else.

- agree but there are also considerations that should be thought about carefully

An effective ethical code doesn't need to — in fact, probably shouldn't — focus on ethical issues. What matters most are the consequences, not the tools we use to bring those consequences about. 
??

not convinced that costly signaling is the way to go

"data scientists don't need a list of ways to be virtuous. They need a list of ways to prove they aren't charlatans"
- it is not a list of ways to be virtuous necessarily---could be things to keep in mind that they hadn't thought about before, and hypothesized and tested ways of doing things (e.g., open data)

also, as soon as you need to engage in costly signaling to get hired, people will just do that

confused what costly signals he is talking about

has a very simplistic view of ethics --- hanna wilson has a more nuanced take},
author = {Wheeler, Schaun},
title = {{An ethical code can't be about ethics – Towards Data Science}},
url = {https://towardsdatascience.com/an-ethical-code-cant-be-about-ethics-66acaea6f16f},
urldate = {2018-05-12}
}
@article{Wickham2014,
annote = {think about all these principles in R terms
- interesting how its difficult to conceive of what's going on without referring to a function
- the data is a material, and we need well designed tools to work with it. those tools have to fit our needs and psychology, and should be easy to use

alternative formulations of tidiness?

psychology aspect
- always kind of astounded how little "ux" like research there is in this field, and how generally uninterested in these topics most work in data science seems to be
- as well as with visualization},
author = {Wickham, Hadley},
doi = {10.18637/jss.v081.b02},
file = {:Users/sarakai24/Downloads/v59i10.pdf:pdf},
isbn = {9783319497501},
journal = {Journal of Statistical Software},
keywords = {chained equations,model diagnostics,multiple imputation,weakly informative},
number = {October},
title = {{Tidy Data}},
volume = {81},
year = {2014}
}
@article{Konold,
annote = {so many choices involved in data analysis
this makes me think more and more that data science should be its own academic field -- rich field for study with},
author = {Konold, Clifford and Finzer, William and Kreetong, Kosoom},
file = {:Users/sarakai24/Downloads/SERJ16(2){\_}Konold.pdf:pdf},
keywords = {data,data modeling,data organization,statistics education research},
number = {January 1985},
pages = {191--212},
title = {{Modeling as a core component of structuring data}},
volume = {16}
}
@article{Parker2017,
abstract = {Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. The process of developing the technical artifact—that is, the paper, dashboard, or other deliverable—is much less frequently taught, presumably because of an aversion to cookbookery or prescribing specific software choices. In this paper I argue that it's critical to teach analysts how to go about developing an analysis in order to maximize the probability that their analysis is reproducible, accurate, and collaborative. A critical component of this is adopting a blameless postmortem culture. By encouraging the use of and fluency in tooling that implements these opinions, as well as a blameless way of correcting course as analysts encounter errors, we as a community can foster the growth of processes that fail the practitioners as infrequently as possible.},
author = {Parker, Hilary},
doi = {10.7287/peerj.preprints.3210v1},
file = {:Users/sarakai24/Downloads/peerj-preprints-3210.pdf:pdf},
issn = {2167-9843},
journal = {PeerJ Preprints},
keywords = {blameless postmortems,data science,opinionated software,python,reproducibility,rstats,software engineering,statistics},
pages = {e3210v1},
title = {{Opinionated analysis development}},
url = {https://doi.org/10.7287/peerj.preprints.3210v1},
volume = {5},
year = {2017}
}
@article{Cobb2007,
abstract = {As we begin the 21st century, the introductory statistics course appears healthy, with its emphasis on real examples, data production, and graphics for exploration and assumption-checking. Without doubt this emphasis marks a major improvement over introductory courses of the 1960s, an improvement made possible by the vaunted “computer revolution.” Nevertheless, I argue that despite broad acceptance and rapid growth in enrollments, the consensus curriculum is still an unwitting prisoner of history. What we teach is largely the technical machinery of numerical approximations based on the normal distribution and its many subsidiary cogs. This machinery was once necessary, because the conceptually simpler alternative based on permutations was computationally beyond our reach. Before computers statisticians had no choice. These days we have no excuse. Randomization-based inference makes a direct connection between data production and the logic of inference that deserves to be at the core of every introductory course. Technology allows us to do more with less: more ideas, less technique. We need to recognize that the computer revolution in statistics education is far from over.},
annote = {are the three R's like bayesian inference? or is it different? unclear if he's proposing something new

summary: current statistics education needs to change. the focus on normal distributions is outdated and only hurts students. instead, we should focus on teaching inference based on randomization. 

he gives reasons why it doesn't make sense to focus on normal distributions, t tests, etc. 

how would proponents of traditional statistics education respond? do they know that the current curriculum is outdated? or do they think that we should carry on for some reason? what do they argue?},
author = {Cobb, George W},
file = {:Users/sarakai24/Downloads/qt6hb3k0nz.pdf:pdf},
isbn = {1933-4214},
issn = {1933-4214},
journal = {Technology Innovations in Statistics Education},
keywords = {computing,curriculum,normal distribution,permutation,randomization,sampling distribution},
number = {1},
pages = {1--16},
title = {{The Introductory Statistics Course: A Ptolemaic Curriculum}},
url = {http://www.escholarship.org/uc/item/6hb3k0nz{\%}5Cnpapers3://publication/uuid/426B647B-BAD9-4278-8F02-990BD513D5FA},
volume = {1},
year = {2007}
}
@article{Bryan2017,
abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources.},
annote = {make a rstudio project for each git repository

stay current with practices 

data science not just statistics -- need to know stuff like git

modern workflows and tooling is very important to data science},
author = {Bryan, Jennifer},
doi = {10.7287/peerj.preprints.3159v2},
file = {:Users/sarakai24/Downloads/peerj-preprints-3159.pdf:pdf},
issn = {2167-9843},
journal = {The American Statistician},
keywords = {Git,GitHub,R Markdown,R language,data science,reproducibility,workflow},
pages = {1--23},
title = {{Excuse me, do you have a moment to talk about version control?}},
url = {https://peerj.com/preprints/3159.pdf{\%}0Ahttps://peerj.com/preprints/3159v2/},
volume = {5},
year = {2017}
}
@misc{ONeil2016,
annote = {- experiments are done a lot in companies but ethics are really considered
- just try to optimize but don't necessarily think about how it will affect the users' lives

- deploying tools to construct fair processes 
- auditing system for algorithms
- data scientist should talk to ethicists},
author = {O'Neil, Cathy},
title = {{The Ethical Data Scientist}},
url = {http://www.slate.com/articles/technology/future{\_}tense/2016/02/how{\_}to{\_}bring{\_}better{\_}ethics{\_}to{\_}data{\_}science.html},
year = {2016}
}
